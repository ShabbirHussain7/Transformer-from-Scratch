{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages/libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  # here input dimension of model = d_model. In PyTorch's implementation embed_dim has been used instead of d_model.\n",
    "  def __init__(self, d_model, num_heads): # x [batch_size, sequence_length, embedding_dim]\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.head_dim = d_model // num_heads # splitting the query, key, value into multiple attention heads\n",
    "    self.d_model = d_model # size of query, key, value vectors\n",
    "\n",
    "    # we only use a single layer to compute all query, key, value  then split them \n",
    "    # vectors and to make our model faster as a single layer requires only one \n",
    "    # matrix multiplication while 3 layers would require 3 such multiplications\n",
    "\n",
    "    self.linear_qkv = nn.Linear(d_model, 3*d_model)\n",
    "    self.linear_output = nn.Linear(d_model, d_model)\n",
    "  \n",
    "  def calculate_weights(self, q, k):\n",
    "    att_weights = torch.matmul(q, k.transpose(-1, -2))\n",
    "    scaled_weights = att_weights / math.sqrt(self.d_model)\n",
    "    return scaled_weights\n",
    "  \n",
    "  def forward(self, x, mask= None):\n",
    "    qkv = self.linear_qkv(x) # batch_size, sequence_length, d_model\n",
    "    batch_size, seq_len, d_model = qkv.size()\n",
    "    qkv = qkv.view(batch_size, seq_len, self.num_heads, 3, self.head_dim).permute(0, 2, 1, 4, 3) \n",
    "    # after permuting = batch_size, num_heads, seq_len, head_dim, 3\n",
    "    q, k, v = qkv.unbind(dim=-1)\n",
    "    weights = self.calculate_weights(q, k)\n",
    "    if mask != None:\n",
    "      weights += mask\n",
    "    weights = F.softmax(weights, dim = -1)\n",
    "    print(weights.shape, v.shape)\n",
    "    # weights =  batch_size, num_heads, seq_len, seq_len\n",
    "    # values  =  batch_size, num_heads, seq_len, head_dim\n",
    "    updated_values = torch.einsum('bnij,bnjk->bnik', weights, v)\n",
    "    updated_values = updated_values.reshape(batch_size, seq_len, self.num_heads * self.head_dim)\n",
    "\n",
    "    output = self.linear_output(updated_values)\n",
    "    return output\n",
    "    \n",
    "def mask_gen(qk):\n",
    "  mask = torch.full(qk.size() , float('-inf'))\n",
    "  mask = torch.triu(mask, diagonal=1)\n",
    "  return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(d_model, max_seq_len=5000):\n",
    "    all_idx = torch.arange(0, d_model, step=2).float()\n",
    "    denominator = torch.pow(10000, all_idx/d_model)\n",
    "    positions = torch.arange(0, max_seq_len).reshape(max_seq_len, 1).float()\n",
    "    sin_idx = torch.sin(positions/denominator)\n",
    "    cos_idx = torch.cos(positions/denominator)\n",
    "    pe = torch.stack((sin_idx, cos_idx)).permute(1, 2, 0).flatten(start_dim=1, end_dim=2)\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class has works only along the last dimension that is along the embedding dimension.\n",
    "# We can make it more general by adding a parameter that computes the mean across batches as well.\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "  def __init__(self, d_model, epsilon = 1e-05):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.epsilon = epsilon\n",
    "    self.gammas = nn.Parameter(torch.ones(d_model))\n",
    "    self.betas =  nn.Parameter(torch.ones(d_model))\n",
    "  \n",
    "  def forward(self, input_tensor):\n",
    "    mean = input_tensor.mean(dim = -1, keepdim = True)\n",
    "    std_dev = torch.sqrt(((input_tensor - mean) ** 2).mean(dim = -1, keepdim = True) + self.epsilon)\n",
    "    normalized = (input_tensor - mean) / std_dev\n",
    "    output = self.gammas * normalized + self.betas\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "  def __init__(self, d_model, hidden, dropout_prob =0.1):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.hidden = hidden\n",
    "    self.linear1 = nn.Linear(d_model, hidden)\n",
    "    self.linear2 = nn.Linear(hidden, d_model)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.dropout = nn.Dropout(p= dropout_prob)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = self.linear1(x)\n",
    "    x = self.dropout(self.relu(x))\n",
    "    x = self.linear2(x)\n",
    "    return x\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "  def __init__(self, d_model, hidden, num_heads, dropout_prob):\n",
    "    super().__init__()\n",
    "    self.mul_head_att = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    self.norm1 = LayerNormalization(d_model)\n",
    "    self.norm2 = LayerNormalization(d_model)\n",
    "\n",
    "    self.ff_layers = FeedForward(d_model, hidden, dropout_prob)\n",
    "\n",
    "    self.dropout1 = nn.Dropout(p=dropout_prob)\n",
    "    self.dropout2 = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "  def forward(self, x):\n",
    "    res_x = x.clone()\n",
    "    x = self.norm1(self.dropout1(self.mul_head_att(x)) + res_x)   \n",
    "    res_x = x.clone()\n",
    "    x = self.norm2(self.dropout2(self.ff_layers (x)) + res_x)\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, d_model, hidden, num_heads, dropout_prob, num_layers):\n",
    "    super().__init__()\n",
    "    self.layers = nn.ModuleList([\n",
    "    EncoderBlock(d_model, hidden, num_heads, dropout_prob)\n",
    "      for _ in range(num_layers)\n",
    "    ])\n",
    "\n",
    "  def forward(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    return x\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
