{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CZN3SXNLGdJL"
      },
      "outputs": [],
      "source": [
        "# Importing packages/libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FoP8Bog3GdJM"
      },
      "outputs": [],
      "source": [
        "def mask_gen(qk):\n",
        "  mask = torch.full(qk.size() , float('-inf'))\n",
        "  mask = torch.triu(mask, diagonal=1)\n",
        "  return mask\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  # here input dimension of model = d_model. In PyTorch's implementation embed_dim has been used instead of d_model.\n",
        "  def __init__(self, d_model, num_heads, cross=False): # x [batch_size, sequence_length, embedding_dim]\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_model // num_heads # splitting the query, key, value into multiple attention heads\n",
        "    self.d_model = d_model # size of query, key, value vectors\n",
        "\n",
        "    # we only use a single layer to compute all query, key, value  then split them \n",
        "    # vectors and to make our model faster as a single layer requires only one \n",
        "    # matrix multiplication while 3 layers would require 3 such multiplications\n",
        "    self.cross = cross # boolean to store whether we are applying self attention or cross attention(as required by the decoder)\n",
        "    if cross:\n",
        "      self.linear_qk = nn.Linear(d_model, 2*d_model)\n",
        "      self.linear_v  = nn.Linear(d_model, d_model)\n",
        "    else:\n",
        "      self.linear_qkv = nn.Linear(d_model, 3*d_model)\n",
        "    self.linear_output = nn.Linear(d_model, d_model)\n",
        "  \n",
        "  def calculate_weights(self, q, k):\n",
        "    att_weights = torch.matmul(q, k.transpose(-1, -2))\n",
        "    scaled_weights = att_weights / math.sqrt(self.d_model)\n",
        "    return scaled_weights\n",
        "  \n",
        "  def forward(self, x, y= None, mask= False):\n",
        "    if not self.cross:\n",
        "      qkv = self.linear_qkv(x) # x = batch_size, sequence_length, d_model\n",
        "    else: \n",
        "      qk = self.linear_qk(x) # x = batch_size, sequence_length, d_model\n",
        "      v  = self.linear_v(y)  # y = batch_size, sequence_length, d_model\n",
        "      qkv = torch.cat((qk, v), dim = -1)\n",
        "    batch_size, seq_len, d_model = qkv.size()\n",
        "    qkv = qkv.view(batch_size, seq_len, self.num_heads, 3, self.head_dim).permute(0, 2, 1, 4, 3) \n",
        "    # after permuting = batch_size, num_heads, seq_len, head_dim, 3\n",
        "    q, k, v = qkv.unbind(dim=-1)\n",
        "    weights = self.calculate_weights(q, k)\n",
        "    if mask:\n",
        "      mask = mask_gen(weights)\n",
        "      print(mask)\n",
        "      weights += mask\n",
        "    weights = F.softmax(weights, dim = -1)\n",
        "    # weights =  batch_size, num_heads, seq_len, seq_len\n",
        "    # values  =  batch_size, num_heads, seq_len, head_dim\n",
        "    updated_values = torch.einsum('bnij,bnjk->bnik', weights, v)\n",
        "    updated_values = updated_values.reshape(batch_size, seq_len, self.num_heads * self.head_dim)\n",
        "\n",
        "    output = self.linear_output(updated_values)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WE2Q0F9qGdJN"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(d_model, max_seq_len=5000):\n",
        "    all_idx = torch.arange(0, d_model, step=2).float()\n",
        "    denominator = torch.pow(10000, all_idx/d_model)\n",
        "    positions = torch.arange(0, max_seq_len).reshape(max_seq_len, 1).float()\n",
        "    sin_idx = torch.sin(positions/denominator)\n",
        "    cos_idx = torch.cos(positions/denominator)\n",
        "    pe = torch.stack((sin_idx, cos_idx)).permute(1, 2, 0).flatten(start_dim=1, end_dim=2)\n",
        "    return pe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lqBIIpHtGdJN"
      },
      "outputs": [],
      "source": [
        "# This class has works only along the last dimension that is along the embedding dimension.\n",
        "# We can make it more general by adding a parameter that computes the mean across batches as well.\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self, d_model, epsilon = 1e-05):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.epsilon = epsilon\n",
        "    self.gammas = nn.Parameter(torch.ones(d_model))\n",
        "    self.betas =  nn.Parameter(torch.ones(d_model))\n",
        "  \n",
        "  def forward(self, input_tensor):\n",
        "    mean = input_tensor.mean(dim = -1, keepdim = True)\n",
        "    std_dev = torch.sqrt(((input_tensor - mean) ** 2).mean(dim = -1, keepdim = True) + self.epsilon)\n",
        "    normalized = (input_tensor - mean) / std_dev\n",
        "    output = self.gammas * normalized + self.betas\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-PLiz-0MGdJN"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, d_model, hidden, dropout_prob =0.1):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.hidden = hidden\n",
        "    self.linear1 = nn.Linear(d_model, hidden)\n",
        "    self.linear2 = nn.Linear(hidden, d_model)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p= dropout_prob)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.dropout(self.relu(x))\n",
        "    x = self.linear2(x)\n",
        "    return x\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sH_cH-h3GdJO"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, d_model, hidden, num_heads, dropout_prob):\n",
        "    super().__init__()\n",
        "    self.mul_head_att = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.norm1 = LayerNormalization(d_model)\n",
        "    self.norm2 = LayerNormalization(d_model)\n",
        "\n",
        "    self.ff_layers = FeedForward(d_model, hidden, dropout_prob)\n",
        "\n",
        "    self.dropout1 = nn.Dropout(p=dropout_prob)\n",
        "    self.dropout2 = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "  def forward(self, x):\n",
        "    res_x = x.clone()\n",
        "    x = self.norm1(self.dropout1(self.mul_head_att(x)) + res_x)   \n",
        "    res_x = x.clone()\n",
        "    x = self.norm2(self.dropout2(self.ff_layers (x)) + res_x)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jnCUp3q7GdJO"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, d_model, hidden, num_heads, dropout_prob, num_layers):\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList([\n",
        "    EncoderBlock(d_model, hidden, num_heads, dropout_prob)\n",
        "      for _ in range(num_layers)\n",
        "    ])\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "V6mgTuWbGdJO"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, d_model, hidden, num_heads, dropout_prob):\n",
        "    super().__init__()\n",
        "\n",
        "    self.mul_head_att1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mul_head_att2 = MultiHeadAttention(d_model, num_heads, True)\n",
        "    \n",
        "    self.norm1 = LayerNormalization(d_model)\n",
        "    self.norm2 = LayerNormalization(d_model)\n",
        "    self.norm3 = LayerNormalization(d_model)\n",
        "\n",
        "    self.ff_layers = FeedForward(d_model, hidden, dropout_prob)\n",
        "\n",
        "    self.dropout1 = nn.Dropout(p=dropout_prob)\n",
        "    self.dropout2 = nn.Dropout(p=dropout_prob)\n",
        "    self.dropout3 = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "  def forward(self, x, encoder_output):\n",
        "    res_x = x.clone()\n",
        "    x = self.norm1(self.dropout1(self.mul_head_att1(x, True)) + res_x)   \n",
        "    \n",
        "    res_x = x.clone()\n",
        "    x = self.norm2(self.dropout2(self.mul_head_att2(x, encoder_output, False)) + res_x)\n",
        "\n",
        "    res_x = x.clone()\n",
        "    x = self.norm3(self.dropout3(self.ff_layers(x)) + res_x)  \n",
        "    return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, d_model, hidden, num_heads, dropout_prob, num_layers):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.layers = nn.ModuleList([DecoderBlock(d_model, hidden, num_heads, dropout_prob) for _ in range(num_layers)])\n",
        "\n",
        "  def forward(self, x, encoder_output):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, encoder_output)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO4ChGpvGdJO",
        "outputId": "3ef4b7ea-f016-4be7-d4e5-85e35a3f00fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: torch.Size([16, 200, 256])\n",
            "Decoder output shape: torch.Size([16, 200, 256])\n"
          ]
        }
      ],
      "source": [
        "d_model = 256\n",
        "num_heads = 8\n",
        "dropout_prob = 0.1\n",
        "batch_size = 16\n",
        "max_sequence_length = 200\n",
        "hidden = 1024\n",
        "num_layers = 1\n",
        "encoder = Encoder(d_model, hidden, num_heads, dropout_prob, num_layers)\n",
        "\n",
        "x = torch.randn( (batch_size, max_sequence_length, d_model) ) # includes positional encoding\n",
        "out = encoder(x)\n",
        "print(f\"Encoder output shape: {out.shape}\")\n",
        "decoder = Decoder(d_model, hidden, num_heads, dropout_prob, num_layers)\n",
        "dout = decoder(x, out)\n",
        "print(f\"Decoder output shape: {dout.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGOjqLCFGdJP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}