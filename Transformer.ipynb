{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages/libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, input_dim, d_model, num_heads): # x [batch_size, sequence_length, embedding_dim]\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.input_dim =  input_dim \n",
    "    self.num_heads = num_heads\n",
    "    self.head_dim = d_model // num_heads # splitting the query, key, value into multiple attention heads\n",
    "    self.d_model = d_model # size of query, key, value vectors\n",
    "\n",
    "    # we only use a single layer to compute all query, key, value  then split them \n",
    "    # vectors and to make our model faster as a single layer requires only one \n",
    "    # matrix multiplication while 3 layers would require 3 such multiplications\n",
    "\n",
    "    self.linear_qkv = nn.Linear(self.input_dim, 3*d_model)\n",
    "    self.linear_output = nn.Linear(d_model, d_model)\n",
    "  \n",
    "  def calculate_weights(self, q, k):\n",
    "    att_weights = torch.matmul(q, k.transpose(-1, -2))\n",
    "    scaled_weights = att_weights / math.sqrt(self.d_model)\n",
    "    return scaled_weights\n",
    "  \n",
    "  def forward(self, x, mask= None):\n",
    "    batch_size, seq_len, input_dim = x.size()\n",
    "    qkv = self.linear_qkv(x) # batch_size, sequence_length, d_model\n",
    "    qkv = qkv.reshape(batch_size, seq_len, self.num_heads, 3*self.head_dim) \n",
    "    qkv = qkv.permute(0,2,1,3) # batch_size, num_heads, seq_len, head_dim (*3 for query, key, value)\n",
    "    q, k, v = torch.split(qkv, qkv.size(-1) // 3, dim=-1)\n",
    "    weights = self.calculate_weights(q, k)\n",
    "    if mask != None:\n",
    "      weights += mask\n",
    "    weights = F.softmax(weights, dim = -1)\n",
    "    updated_values = torch.matmul(weights, v)\n",
    "    updated_values = updated_values.reshape(batch_size, seq_len, self.num_heads * self.head_dim)\n",
    "    output = self.linear_output(updated_values)\n",
    "    return output\n",
    "    \n",
    "def mask_gen(qk):\n",
    "  mask = torch.full(qk.size() , float('-inf'))\n",
    "  mask = torch.triu(mask, diagonal=1)\n",
    "  return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(d_model, max_seq_len=5000):\n",
    "    all_idx = torch.arange(0, d_model, step=2).float()\n",
    "    denominator = torch.pow(10000, all_idx/d_model)\n",
    "    positions = torch.arange(0, max_seq_len).reshape(max_seq_len, 1).float()\n",
    "    sin_idx = torch.sin(positions/denominator)\n",
    "    cos_idx = torch.cos(positions/denominator)\n",
    "    pe = torch.stack((sin_idx, cos_idx)).permute(1, 2, 0).flatten(start_dim=1, end_dim=2)\n",
    "    return pe\n",
    "\n",
    "pe = positional_encoding(d_model=6, max_seq_len=10)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
