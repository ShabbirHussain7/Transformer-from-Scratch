{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "CZN3SXNLGdJL"
      },
      "outputs": [],
      "source": [
        "# Importing packages/libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def positional_encoding(d_model, max_seq_len=5000):\n",
        "  all_idx = torch.arange(0, d_model, step=2).float()\n",
        "  denominator = torch.pow(10000, all_idx/d_model)\n",
        "  positions = torch.arange(0, max_seq_len).reshape(max_seq_len, 1).float()\n",
        "  sin_idx = torch.sin(positions/denominator)\n",
        "  cos_idx = torch.cos(positions/denominator)\n",
        "  pe = torch.stack((sin_idx, cos_idx)).permute(1, 2, 0).flatten(start_dim=1, end_dim=2)\n",
        "  return pe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "FoP8Bog3GdJM"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  # here input dimension of model = d_model. In PyTorch's implementation embed_dim has been used instead of d_model.\n",
        "  def __init__(self, d_model, num_heads, cross=False): # x [batch_size, sequence_length, embedding_dim]\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_model // num_heads # splitting the query, key, value into multiple attention heads\n",
        "    self.d_model = d_model # size of query, key, value vectors\n",
        "\n",
        "    # we only use a single layer to compute all query, key, value  then split them \n",
        "    # vectors and to make our model faster as a single layer requires only one \n",
        "    # matrix multiplication while 3 layers would require 3 such multiplications\n",
        "    self.cross = cross # boolean to store whether we are applying self attention or cross attention(as required by the decoder)\n",
        "    if cross:\n",
        "      self.linear_qk = nn.Linear(d_model, 2*d_model)\n",
        "      self.linear_v  = nn.Linear(d_model, d_model)\n",
        "    else:\n",
        "      self.linear_qkv = nn.Linear(d_model, 3*d_model)\n",
        "    self.linear_output = nn.Linear(d_model, d_model)\n",
        "  \n",
        "  def calculate_weights(self, q, k):\n",
        "    att_weights = torch.matmul(q, k.transpose(-1, -2))\n",
        "    scaled_weights = att_weights / math.sqrt(self.d_model)\n",
        "    return scaled_weights\n",
        "  \n",
        "  def forward(self, x, y= None, mask= None):\n",
        "    if not self.cross:\n",
        "      qkv = self.linear_qkv(x) # x = batch_size, sequence_length, d_model\n",
        "    else: \n",
        "      qk = self.linear_qk(y) # x = batch_size, sequence_length, d_model\n",
        "      v  = self.linear_v(x)  # y = batch_size, sequence_length, d_model\n",
        "      qkv = torch.cat((qk, v), dim = -1)\n",
        "    batch_size, seq_len, d_model = qkv.size()\n",
        "    qkv = qkv.view(batch_size, seq_len, self.num_heads, 3, self.head_dim).permute(0, 2, 1, 4, 3) \n",
        "    # after permuting = batch_size, num_heads, seq_len, head_dim, 3\n",
        "    q, k, v = qkv.unbind(dim=-1)\n",
        "    weights = self.calculate_weights(q, k)\n",
        "\n",
        "    if mask is not None:\n",
        "      weights = weights.masked_fill(mask == 0, -1e9)\n",
        "    weights = torch.softmax(weights, dim=-1)\n",
        "\n",
        "    # weights =  batch_size, num_heads, seq_len, seq_len\n",
        "    # values  =  batch_size, num_heads, seq_len, head_dim\n",
        "    updated_values = torch.einsum('bnij,bnjk->bnik', weights, v)\n",
        "    updated_values = updated_values.reshape(batch_size, seq_len, self.num_heads * self.head_dim)\n",
        "\n",
        "    output = self.linear_output(updated_values)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "lqBIIpHtGdJN"
      },
      "outputs": [],
      "source": [
        "# This class has works only along the last dimension that is along the embedding dimension.\n",
        "# We can make it more general by adding a parameter that computes the mean across batches as well.\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self, d_model, epsilon = 1e-05):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.epsilon = epsilon\n",
        "    self.gammas = nn.Parameter(torch.ones(d_model))\n",
        "    self.betas =  nn.Parameter(torch.ones(d_model))\n",
        "  \n",
        "  def forward(self, input_tensor):\n",
        "    mean = input_tensor.mean(dim = -1, keepdim = True)\n",
        "    std_dev = torch.sqrt(((input_tensor - mean) ** 2).mean(dim = -1, keepdim = True) + self.epsilon)\n",
        "    normalized = (input_tensor - mean) / std_dev\n",
        "    output = self.gammas * normalized + self.betas\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "-PLiz-0MGdJN"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, d_model, hidden, dropout_prob =0.1):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.hidden = hidden\n",
        "    self.linear1 = nn.Linear(d_model, hidden)\n",
        "    self.linear2 = nn.Linear(hidden, d_model)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p= dropout_prob)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.dropout(self.relu(x))\n",
        "    x = self.linear2(x)\n",
        "    return x\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "sH_cH-h3GdJO"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, d_model, hidden, num_heads, dropout_prob):\n",
        "    super().__init__()\n",
        "    self.mul_head_att = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.norm1 = LayerNormalization(d_model)\n",
        "    self.norm2 = LayerNormalization(d_model)\n",
        "\n",
        "    self.ff_layers = FeedForward(d_model, hidden, dropout_prob)\n",
        "\n",
        "    self.dropout1 = nn.Dropout(p=dropout_prob)\n",
        "    self.dropout2 = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    res_x = x.clone()\n",
        "    x = self.norm1(self.dropout1(self.mul_head_att(x, mask)) + res_x)   \n",
        "    res_x = x.clone()\n",
        "    x = self.norm2(self.dropout2(self.ff_layers (x)) + res_x)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "jnCUp3q7GdJO"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, d_model, hidden, num_heads, dropout_prob, num_layers, vocab_size, max_seq_len):\n",
        "    super().__init__()\n",
        "    self.max_seq_len = max_seq_len\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.dropout = nn.Dropout(p=dropout_prob)\n",
        "    \n",
        "    self.layers = nn.ModuleList([\n",
        "    EncoderBlock(d_model, hidden, num_heads, dropout_prob)\n",
        "      for _ in range(num_layers)\n",
        "    ])\n",
        "\n",
        "  def forward(self, x, mask_encoder):\n",
        "    x = self.dropout(self.embedding(x) + positional_encoding(self.d_model, self.max_seq_len))\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask_encoder)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "V6mgTuWbGdJO"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, d_model, hidden, num_heads, dropout_prob):\n",
        "    super().__init__()\n",
        "\n",
        "    self.mul_head_att1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mul_head_att2 = MultiHeadAttention(d_model, num_heads, True)\n",
        "    \n",
        "    self.norm1 = LayerNormalization(d_model)\n",
        "    self.norm2 = LayerNormalization(d_model)\n",
        "    self.norm3 = LayerNormalization(d_model)\n",
        "\n",
        "    self.ff_layers = FeedForward(d_model, hidden, dropout_prob)\n",
        "\n",
        "    self.dropout1 = nn.Dropout(p=dropout_prob)\n",
        "    self.dropout2 = nn.Dropout(p=dropout_prob)\n",
        "    self.dropout3 = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "  def forward(self, x, encoder_output, mask_encoder, mask_decoder):\n",
        "    res_x = x.clone()\n",
        "    x = self.norm1(self.dropout1(self.mul_head_att1(x=x, mask= mask_decoder)) + res_x)   \n",
        "    \n",
        "    res_x = x.clone()\n",
        "    x = self.norm2(self.dropout2(self.mul_head_att2(x=x, y=encoder_output, mask = mask_encoder)) + res_x)\n",
        "\n",
        "    res_x = x.clone()\n",
        "    x = self.norm3(self.dropout3(self.ff_layers(x)) + res_x)  \n",
        "    return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, d_model, hidden, num_heads, dropout_prob, num_layers, vocab_size, max_seq_len):\n",
        "    super().__init__()\n",
        "    self.max_seq_len = max_seq_len\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.dropout = nn.Dropout(p=dropout_prob)\n",
        "    self.layers = nn.ModuleList([DecoderBlock(d_model, hidden, num_heads, dropout_prob) for _ in range(num_layers)])\n",
        "\n",
        "  def forward(self, x, encoder_output, mask_encoder, mask_decoder):\n",
        "    x = self.dropout(self.embedding(x) + positional_encoding(self.d_model, self.max_seq_len))\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, encoder_output, mask_encoder, mask_decoder)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "oGOjqLCFGdJP"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self,d_model, hidden, num_heads, dropout_prob, num_layers, input_vocab_size, output_vocab_size, max_seq_len):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.max_seq_len = max_seq_len\n",
        "    self.dropout = nn.Dropout(p = dropout_prob)\n",
        "    self.encoder = Encoder(d_model, hidden, num_heads, dropout_prob, num_layers, input_vocab_size, max_seq_len)\n",
        "    self.decoder = Decoder(d_model, hidden, num_heads, dropout_prob, num_layers, output_vocab_size, max_seq_len)\n",
        "    self.linear = nn.Linear(d_model, output_vocab_size)\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "  # x will be tokenized sentences. The tokenization can be done by using some library such as nltk, spacy etc.\n",
        "  def forward(self, x, y):\n",
        "    x_mask = self.mask_gen(x, self.max_seq_len, False)\n",
        "    y_mask = self.mask_gen(y, self.max_seq_len, True)\n",
        "    encoder_output = self.encoder(x, x_mask)\n",
        "    output = self.softmax(self.linear(self.decoder(y, encoder_output, x_mask, y_mask)))\n",
        "    return output\n",
        "    \n",
        "  def mask_gen(self, x, max_seq_len, lookahead):   \n",
        "    if lookahead:\n",
        "      mask = (x != 0).unsqueeze(1).unsqueeze(3)\n",
        "      lookahead_mask = (1 - torch.triu(torch.ones(1, max_seq_len, max_seq_len), diagonal=1)).bool()\n",
        "      mask = mask & lookahead_mask\n",
        "    else:\n",
        "      mask = (x != 0).unsqueeze(1).unsqueeze(2)\n",
        "    return mask\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQT1MtAr8YCQ",
        "outputId": "82fa3382-f9c3-474e-e461-dc7bc79be2ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 200, 156])\n"
          ]
        }
      ],
      "source": [
        "d_model = 256\n",
        "num_heads = 8\n",
        "dropout_prob = 0.1\n",
        "batch_size = 16\n",
        "max_sequence_length = 200\n",
        "hidden = 1024\n",
        "num_layers = 1\n",
        "input_vocab_size = 200\n",
        "output_vocab_size = 156\n",
        "\n",
        "# x is input while y is output\n",
        "x = torch.randint(1, input_vocab_size, (batch_size, max_sequence_length))  # (batch_size, seq_length), The generated random integers will be within the range [1, input_vocab_size).\n",
        "y = torch.randint(1, output_vocab_size, (batch_size, max_sequence_length))  # (batch_size, seq_length), The generated random integers will be within the range [1, output_vocab_size).\n",
        "\n",
        "transformer = Transformer(d_model, hidden, num_heads, dropout_prob, num_layers, input_vocab_size, output_vocab_size, max_sequence_length)\n",
        "output = transformer(x,y)\n",
        "print(output.shape) # (batch_size, max_seq_length, output_vocab_size)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
